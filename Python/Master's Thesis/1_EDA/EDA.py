# -*- coding: utf-8 -*-
"""FP_EDA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-E6H2AuRU-J5qbGF18bVUBicTaIeX066
"""

#匯入套件
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#載入資料
original_data = pd.read_csv("trainingData.csv")

#檢視original原始資料
original_data

#檢視original原始資料中的前5筆資料
original_data.head()

#檢視original原始資料的基本統計分析
original_data.describe()

#將original原始資料分成特徵(X)和標籤(y)
X = original_data.iloc[:,:520]

y = original_data.iloc[:,520:526]

#將超出AP範圍的值替換成nan，以避免干擾對AP範圍內RSSI分佈的分析
X = (X.replace(to_replace=100,value=np.nan))

#查看WAP001到WAP520中RSSI值分布情況
X_stack = X.stack(dropna=False)
sns.distplot(X_stack.dropna(),kde = False)

'''
在上圖中，大多數 RSSI 值存在於 (-95dBm,-80dBm) 範圍內。 顯然，分佈是右偏的，分佈左側的大多數值。

有趣的是，最高的 RSSI 值為 0 dBm，這顯然是一個異常值。 (RSSI值為0表示沒有收到任何Wi-Fi資料)
通常，當一個設備從多個 AP 接收數據時，它會提供與AP關聯最高的RSSI值。
因此，接下來分析每個樣本的最高 RSSI 的分佈。
'''

#查看每次測量中最高的RSSI值分布情況
X_ap_max = (X
           .max(axis = 1,skipna=True)
           .dropna())

fig, ax = plt.subplots(1,1)

sns.distplot(X_ap_max.dropna(), ax = ax,kde=False)
ax.set_xlabel("Highest RSSI per measurement")

'''
偏度：對於正態分佈的數據，偏度應該在 0 左右。偏度值 > 0 意味著分佈的左尾有更多的權重。 同樣，負值表示左偏分佈，右尾權重更大。
'''

print("Skewness of entire RSSI distribution", X_stack.skew())
print("Skewness of max RSSI distribution", X_ap_max.skew())

'''
預計，每個樣本的最大 RSSI 分佈的中心高於整個分佈。 
此外，它看起來更正常，這可以通過將偏度值從 1.155 減少到 0.896 來驗證。
'''

'''
代表在每個WAP中每個測量範圍內只有幾個 AP (2~3個)。
'''

aps_in_range = (X
                 .notnull()
                 .sum(axis = 1))

fig, ax = plt.subplots(1,1)

sns.violinplot(aps_in_range, ax = ax)
ax.set_xlabel("Number of APs in range")

'''
有趣的是，大多數樣本的範圍內有超過 13 個 AP，最多有 51 個 AP。 
儘管 51 是最大值，但根據小提琴圖中顯示的密度，它顯然是一個異常值。

13 個 AP 對應於我們數據集中所有 AP 的不到 3%。(13/520=0.025)
'''

'''
我們確實觀察了一些範圍內有 0 個 AP 的訓練樣本。 讓我們從訓練數據中刪除這些樣本。
'''

#刪除在WAP001到WAP520中有0的訓練樣本
print("Before sample removal:", len(X))

y = (y.loc[X.notnull().any(axis=1),:])

X = (X.loc[X.notnull().any(axis=1),:])

print("After sample removal:", len(X))

'''
我們可以刪除與我們的任何訓練樣本中不在範圍內的 AP 相關的 RSSI 列，因為該數據中不包含有用的信息。
'''

#移除WAP001到WAP520特徵中全部為空值的特徵
all_nan = (X.isnull().all(axis=0) == False)
filtered_cols = (all_nan[all_nan]
                 .index
                 .values)

print("Before removing predictors with no in-range values", X.shape)

X = X.loc[:,filtered_cols]

print("After removing predictors with no in-range values", X.shape)

'''
缺失值的處理

在應用任何機器學習模型之前，我們需要填寫缺失值。 我們如何填充缺失值？ 
我們是否應該簡單地將其等同於數據集中的絕對最小值？ 
或者，我們是否應該估計每個預測變量的分佈並從左尾隨機抽樣？
'''

#有多少缺失值(百分比)
# Proportion of out of range values
sum(X_stack.isnull() == 0)/len(X_stack)

'''
在WAP001到WAP520的RSSI值中有 96.1% 表示超出範圍。 
這是預期的，因為對於任何給定的測量，只有一小部分 AP 位於移動設備位置的範圍內，正如我們在上一節中觀察到的那樣。
'''

#另一種缺失值-預測變量的缺失值數據百分比
miss_perc = (X.isnull().sum(axis=0))

miss_perc *= 100/len(X)

sns.distplot(miss_perc,bins = 50,kde=False)

'''
有趣的是，160 個預測變量有超過 99.5% 的值缺失！ 
我從 /u/DrLionelRaymond 那裡得到了關於如何填充缺失值的有用建議。 
當缺失數據量較低時，簡單的單值插補（例如，絕對最小值）很好，
但對於超過 85% 的缺失數據，使用單值可能會引入顯著偏差。
一種方法是基於最大似然估計器的插補，它確實更穩健，但它的性能在缺失率接近 80% 時下降了很多。
'''

'''
Rubin 發表了大量關於基於分佈的方法的文章，Chen 在貝葉斯方法方面做了很多工作。 
重要的是要指出，在執行插補時，您希望插補值與基礎分佈相匹配。
'''

'''
為此，了解我們的預測變量的分佈很重要。 
零假設是每個預測變量都具有正態分佈。 
讓我們使用以下度量，即偏度和峰度來找出分佈與正態的距離。
'''

'''
偏度Skewness：
對於正態分佈的數據，偏度應該在 0 左右。
偏度值 > 0 意味著分佈的左尾有更多的權重。 
同樣，負值表示左偏分佈，右尾權重更大。
'''

'''
峰度Kurtosis：
峰度是第四個中心矩除以方差的平方。 
如果分佈具有正峰度，則意味著它的尾部比正態分佈更多。 
類似地，如果分佈具有負峰度，則它的尾部比正態分佈少。
'''

# Skewness of the predictors ignoring out-of-range values
X_skew = X.skew()
X_kurtosis = X.kurtosis()

g = sns.jointplot(y=X_kurtosis, x=X_skew)
g.set_axis_labels('Skewness','Kurtosis')

'''
顯然，大多數預測變量的偏度和峰度值都在 (-1,1) 範圍內，表明接近正態性。 
基於此，我們可以為每個預測變量估計一個高斯核來填充缺失值。
'''

'''
核密度估計Kernel Density Estimation
#把長條圖畫成折線圖！

#查看WAP001到WAP520中RSSI值分布情況(上面的長條圖轉成折線圖)
'''

from scipy.stats.distributions import norm
from sklearn.neighbors import KernelDensity

# The grid we'll use for plotting
x_grid = np.linspace(-100, 0, 100)

# Draw points from a bimodal distribution in 1D
np.random.seed(0)
x = np.array(X.stack().dropna())

kde_skl = KernelDensity(bandwidth=1.0,rtol=1e-4)
kde_skl.fit(x[:, np.newaxis])
# score_samples() returns the log-likelihood of the samples
log_pdf = kde_skl.score_samples(x_grid[:, np.newaxis])
plt.plot(x_grid, np.exp(log_pdf), color = 'blue',alpha = 0.5, lw = 3)
#plt.fill(x_grid, pdf_true,ec='gray',fc='gray',alpha=0.4)

